import os
import wx
import time
import hashlib
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import functools
from src2.file_hasher import FileHasher
import collections
from src.log_manager import log_manager


class FileFinder:
    def __init__(self, folder_path, exclude_folders=None, progress_bar=None,
                 progress_label=None, log_callback=None, ui_update_callback=None,
                 cache_file=None, batch_size=1000):
        self.folder_path = folder_path
        self.exclude_folders = [os.path.normpath(f) for f in (exclude_folders or [])]
        self.progress_bar = progress_bar
        self.progress_label = progress_label
        self.log_callback = log_callback
        self.ui_update_callback = ui_update_callback
        self.hasher = FileHasher()
        self.batch_size = batch_size

        # Ù„ÛŒØ³Øª ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´
        self.supported_extensions = {
            '.mp3', '.flac', '.wav', '.aac', '.ogg', '.m4a', '.wma',
            '.mp4', '.avi', '.mkv', '.mov', '.wmv',
            '.jpg', '.jpeg', '.png', '.gif', '.bmp',
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.txt'
        }

        self.logger = log_manager.get_logger("FileFinder")
        self.logger.info(f"FileFinder Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ø¨Ø±Ø§ÛŒ: {folder_path}")

    def _is_child_process(self):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØ§ Ø¯Ø± ÙØ±Ø§ÛŒÙ†Ø¯ child Ù‡Ø³ØªÛŒÙ…"""
        import multiprocessing
        try:
            return multiprocessing.current_process().name != 'MainProcess'
        except:
            return True

    def _log(self, message, level='info'):
        """Log messages with callback support"""
        if self.log_callback:
            wx.CallAfter(self.log_callback, message)

        if self.logger:
            if level == 'info':
                self.logger.info(message)
            elif level == 'warning':
                self.logger.warning(message)
            elif level == 'error':
                self.logger.error(message)
            elif level == 'debug':
                self.logger.debug(message)

    def _should_skip(self, root):
        """Check if path should be skipped"""
        current_path = os.path.normpath(root)

        # Ù¾Ø±Ø´ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…ÛŒ
        if any(skip in current_path.lower() for skip in ['system', 'windows', '$recycle.bin', 'appdata']):
            return True

        for excluded in self.exclude_folders:
            if current_path.startswith(excluded):
                return True
        return False

    def _is_supported_file(self, filename):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØ§ ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯"""
        ext = os.path.splitext(filename)[1].lower()
        return ext in self.supported_extensions

    def _update_ui(self, total, scanned, speed, duplicates_found, status=""):
        """Update UI with Persian formatted messages"""
        if self.ui_update_callback:
            wx.CallAfter(self.ui_update_callback, {
                'total_files': total,
                'scanned_files': scanned,
                'speed': speed,
                'remaining': total - scanned,
                'percentage': (scanned / total) * 100 if total > 0 else 0,
                'duplicates': duplicates_found,
                'message': f"Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {scanned}/{total} | Ø³Ø±Ø¹Øª: {speed:.1f} ÙØ§ÛŒÙ„/Ø«Ø§Ù†ÛŒÙ‡ | ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§: {duplicates_found}",
                'status': status
            })

    def _build_size_map_optimized(self):
        """Ø³Ø§Ø®Øª Ù†Ù‚Ø´Ù‡ Ø³Ø§ÛŒØ² Ø¨Ø§ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¨Ù‡ØªØ±"""
        size_map = collections.defaultdict(list)
        total_files = 0
        supported_files = 0

        self._log("ðŸ“‚ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³Ú©Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§...")

        start_time = time.time()

        try:
            for root, dirs, files in os.walk(self.folder_path):
                # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ù…Ø§Ù† Ù„Ø­Ø¸Ù‡
                dirs[:] = [d for d in dirs if not self._should_skip(os.path.join(root, d))]

                if self._should_skip(root):
                    continue

                for filename in files:
                    # ÙÛŒÙ„ØªØ± ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡
                    if not self._is_supported_file(filename):
                        continue

                    file_path = os.path.join(root, filename)
                    if not self.hasher.check_file_health(file_path):
                        continue

                    try:
                        file_size = os.path.getsize(file_path)
                        size_map[file_size].append(file_path)
                        total_files += 1
                        supported_files += 1
                    except OSError:
                        continue

            elapsed = time.time() - start_time
            self._log(f"âœ… Ø§Ø³Ú©Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ø²Ù…Ø§Ù†: {elapsed:.2f} Ø«Ø§Ù†ÛŒÙ‡")
            self._log(f"ðŸ“Š Ø¢Ù…Ø§Ø±: {total_files} ÙØ§ÛŒÙ„ Ú©Ù„ØŒ {supported_files} ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡")

        except Exception as e:
            self._log(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {str(e)}", 'error')
            raise

        return size_map, total_files

    def _process_batch(self, file_batch):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ø¯Ø³ØªÙ‡ ÙØ§ÛŒÙ„ - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡"""
        results = []
        for file_path in file_batch:
            try:
                file_hash = self.hasher.hash_file(file_path)
                if file_hash:  # ÙÙ‚Ø· Ø§Ú¯Ø± Ù‡Ø´ Ù…Ø¹ØªØ¨Ø± Ø¨Ø§Ø´Ø¯
                    results.append((file_hash, file_path))
            except Exception as e:
                self.logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {file_path}: {e}")
                continue
        return results

    def find_files(self):
        """Main method to find duplicate files"""
        start_time = time.time()
        self._log("ðŸ” Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ ÛŒØ§ÙØªÙ† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ...")

        try:
            # First pass - count files and build size map
            size_map, total_files = self._build_size_map_optimized()

            if total_files == 0:
                self._log("âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒØ§ÙØª Ù†Ø´Ø¯")
                return []

            # Only process files that have size duplicates
            candidate_files = []
            duplicate_size_count = 0

            for size, files in size_map.items():
                if len(files) > 1:
                    candidate_files.extend(files)
                    duplicate_size_count += len(files)

            self._log(f"ðŸ“Š ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†Ø¯ÛŒØ¯: {len(candidate_files)} Ø§Ø² {duplicate_size_count} ÙØ§ÛŒÙ„ Ø¨Ø§ Ø³Ø§ÛŒØ² ØªÚ©Ø±Ø§Ø±ÛŒ")

            if not candidate_files:
                self._log("âœ… Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ú©Ø§Ù†Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´ ÛŒØ§ÙØª Ù†Ø´Ø¯")
                return []

            # Process files in parallel with progress reporting
            file_hashes = {}
            last_update = time.time()

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ workerÙ‡Ø§
            cpu_count = os.cpu_count() or 1
            max_workers = min(cpu_count, 8)  # Ø­Ø¯Ø§Ú©Ø«Ø± 8 worker

            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                process_func = functools.partial(self._process_file_wrapper)

                # Ø§ÛŒØ¬Ø§Ø¯ progress bar
                with tqdm(total=len(candidate_files), desc="Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø´", unit="ÙØ§ÛŒÙ„",
                          bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{rate_fmt}]") as pbar:

                    futures = {executor.submit(process_func, file_path): file_path
                               for file_path in candidate_files}

                    for future in as_completed(futures):
                        try:
                            file_hash, file_path = future.result(timeout=30)
                            if file_hash and file_path:
                                file_hashes.setdefault(file_hash, []).append(file_path)
                        except Exception as e:
                            self.logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„: {e}")

                        pbar.update(1)

                        # Ø¢Ù¾Ø¯ÛŒØª UI Ù‡Ø± 0.5 Ø«Ø§Ù†ÛŒÙ‡
                        current_time = time.time()
                        if current_time - last_update > 0.5:
                            speed = pbar.n / (current_time - start_time)
                            duplicates = sum(1 for group in file_hashes.values() if len(group) > 1)
                            self._update_ui(len(candidate_files), pbar.n, speed, duplicates,
                                            f"Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø´: {pbar.n}/{len(candidate_files)}")
                            last_update = current_time

            # Final results
            duplicate_groups = [group for group in file_hashes.values() if len(group) > 1]

            # Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ù†ØªØ§ÛŒØ¬
            elapsed_time = time.time() - start_time
            self._log(f"âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ø²Ù…Ø§Ù†: {elapsed_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
            self._log(f"ðŸ“Š Ù†ØªØ§ÛŒØ¬: {len(duplicate_groups)} Ú¯Ø±ÙˆÙ‡ ØªÚ©Ø±Ø§Ø±ÛŒ ÛŒØ§ÙØª Ø´Ø¯")

            if duplicate_groups:
                total_duplicates = sum(len(g) - 1 for g in duplicate_groups)
                self._log(f"ðŸ—‘ï¸  ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ù‚Ø§Ø¨Ù„ Ø­Ø°Ù: {total_duplicates}")

            return duplicate_groups

        except Exception as e:
            self._log(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ ÛŒØ§ÙØªÙ† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ: {str(e)}", 'error')
            raise

    def _process_file_wrapper(self, file_path):
        """Wrapper for parallel processing"""
        try:
            return self.hasher.hash_file(file_path), file_path
        except Exception as e:
            self.logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± wrapper Ø¨Ø±Ø§ÛŒ {file_path}: {e}")
            return None, None

    def find_files_quick(self):
        """Ù†Ø³Ø®Ù‡ Ø³Ø±ÛŒØ¹ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡"""
        size_map, total_files = self._build_size_map_optimized()

        # ÙÙ‚Ø· ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø³Ø§ÛŒØ² ÛŒÚ©Ø³Ø§Ù† Ø¯Ø§Ø±Ù†Ø¯
        potential_duplicates = []
        for size, files in size_map.items():
            if len(files) > 1:
                # Ø§Ú¯Ø± Ø³Ø§ÛŒØ² Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø­ØªÙ…Ø§Ù„ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù† Ø²ÛŒØ§Ø¯ Ø§Ø³Øª
                potential_duplicates.append(files)

        return potential_duplicates